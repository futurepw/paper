Spider类定义了如何爬取某个(或某些)网站。包括了爬取的动作(例如:是否跟进链接)以及如何从网页的内容中提取结构化数据(爬取item)。 换句话说，Spider就是您定义爬取的动作及分析某个网页(或者是有些网页)的地方。

对spider来说，爬取的循环类似下文:

1、以初始的URL初始化Request，并设置回调函数。 当该request下载完毕并返回时，将生成response，并作为参数传给该回调函数。
spider中初始的request是通过调用 start_requests() 来获取的。 start_requests() 读取 start_urls 中的URL， 并以 parse 为回调函数生成 Request 。
2、在回调函数内分析返回的(网页)内容，返回 Item 对象或者 Request 或者一个包括二者的可迭代容器。 返回的Request对象之后会经过Scrapy处理，下载相应的内容，并调用设置的callback函数(函数可相同)。
3、在回调函数内，您可以使用 选择器(Selectors) (您也可以使用BeautifulSoup, lxml 或者您想用的任何解析器) 来分析网页内容，并根据分析的数据生成item。
4、最后，由spider返回的item将被存到数据库(由某些 Item Pipeline 处理)或使用 Feed exports 存入到文件中。

详见：http://docs.pythontab.com/scrapy/scrapy0.24/topics/spiders.html

name
allow_domains
start_urls
custom_settings:覆写setting中的参数，使用字典形式
crawler
settings
from_crawler 可以拿到setting的相关配置
start_requests() 利用start_urls()中的url发送get请求，如果想要post请求，需要进行改写:

def start_requests(self):
    yield scrapy.Request(url='http://httpbin.org/post',method='POST',callback=self.parse_post)

make_requests_from_url():改变初始回调函数，是被start_requests函数调用的，如果也重写了start_requests，这个函数不会被调用
def make_requests_from_url(self,url):
    return scrapy.Request(url=url,callback=self.parse_index)


parse:默认的回调函数，返回一个request或者item


log:scrapy.log.log()

logger:输出日志信息
        self.logger.info(msg)

closed(reason)
